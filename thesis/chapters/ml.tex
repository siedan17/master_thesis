
\section{Machine Learning Modelle}
\label{sec:ml}

In P1 und P2 ist das Ziel, aus Daten der Vergangenheit Informationen zu gewinnen, und diese f\"ur Vorhersagen in die Zunkunft zu verwenden.
Man versucht gewisse Muster in den Daten der Studierenden zu erkennen und anhand dieser eine Aussage \"uber deren Pr\"ufungsaktivit\"at in den kommenden Jahren zu treffen.
Diese Aufgabe ist f\"ur Menschen aufgrund der gro{\ss}en Anzahl an Daten schwer bew\"altigbar. Aus diesem Grund werden Machine Learning Modelle verwendet,
um dieses Ziel zu erreichen. \\

In dieser Arbeit werden ausschlie{\ss}lich Machine Learning Modelle verwendet, die man als \textit{supervised learning} bezeichnet \cite{shalev}. 
Das bedeuted, man verf\"ugt \"uber Daten der Form $(\mathbf{x}_i, y_i)$ mit $i = 1,\dots,n$. Dabei stellt $\mathbf{x}_i \in \mathbb{R}^d$ den Eigenschaftsvektor 
der Inputdaten und $y_i \in \mathbb{R}$ den bereits vorhandenen, tats\"achlichen Zielwert (oder Output) dar. Die Anzahl der Daten wird mit $n$ beschrieben. Alle Daten mit denen der 
Sch\"atzer optimal gebildet werden soll, werden als \textit{Trainingsdaten} bezeichnet. 
Anhand der bekannten Outputs zu den jeweiligen Inputs kann das Modell durch einen Trainingsalgorithmus mit jedem Beispiel verbessert werden.
Man spricht dabei vom \textit{Training} des Modells \cite{shalev}. \\

Das Ziel eines jeden Machine Learning Modells ist es, eine passende Regel zu erkennen, die nicht nur m\"oglichst viele Trainigsdaten richtig abbilden kann, sondern vor allem 
gut auf neue, unbekannte Daten generalisiert \cite{strang}. \\

Die Inputdaten in der vorliegenden Problemstellung sind die Eigenschaftsvektoren der individuellen Studierenden in dem jeweiligen Studienjahr. Die Outputklassen oder 
Outputwerte sind entweder die Klassifizierung \textit{pr\"ufungsaktiv} oder \textit{nicht pr\"ufungsaktiv} oder die erreichten \textit{ECTS pro Jahr}. 
In den folgenden Abs\"atzen wird Machine Learning genauer erkl\"art und formal beschrieben. \\



Wir bezeichnen $\mathcal{X}$ als Menge der Inputdaten und $\mathcal{Y}$ als Menge der Outputdaten. Es wird eine unbekannte Verteilung $\mathcal{D_X}$ \"uber 
$\mathcal{X}$ und zus\"atzlich eine Funktion $f(\cdot)$ angenommen mit der $Y = f(\mathbf{X}) + \epsilon$ gilt, wobei $\epsilon$ als \textit{Rauschen} bezeichnet wird. 
Dabei wird $\epsilon$ f\"ur jeden Datenpunkt als unabh\"angig und identisch verteilt angenommen mit
$\mathbb{E}[\epsilon] = 0$, $\operatorname{Var}(\epsilon) = \sigma^2 \geq 0$.
Um Trainingsdaten zu bekommen, wird eine zuf\"allige Stichprobe $S = \{(\mathbf{x}_1, y_1), \dots ,
(\mathbf{x}_n, y_n)\}$ mit $n$ Datenpunkten gebildet. Es werden alle $\mathbf{x}_i$ aus der Verteilung $\mathcal{D}$ gezogen und es wird 
angenommen, dass $y_i = f(\mathbf{x}_i) + \epsilon_i $ gilt. \\


Wenn $\mathbf{X}$ eine Zufallsvariable mit Verteilung $\mathcal{D}_{\mathcal{X}}$ ist, und $\epsilon$ eine Zufallsvariable ist, dann wird durch 
$Y = f(\mathbf{X}) + \epsilon$ eine weitere Zufallsvariable definiert. Wir nehmen also an, dass $S$ eine n-elementige Stichprobe der Verteilung
von $(\mathbf{X},Y)$ ist. Wenn $Y$ eine diskrete Verteilung hat, spricht man von einem Klassifizierungsproblem, sonst von einem Regressionsproblem.
Die gemeinsame Verteilung von $\mathbf{X}$ und $Y$ nennen wir $\mathcal{D}$.\\

Ausgehend davon soll eine Vorhersagefunktion $h(\cdot;\mathbf{w})$ innerhalb einer parametrisierten Funktionenklasse $\mathcal{H} = \{ h(\cdot; \mathbf{w})
|\mathbf{w} \in \mathbf{W}\}$ gefunden werden. $h_S$ h\"angt zum einen von der Wahl der Funktionenklasse und zum anderen von der Auswahlmethode aus dieser Klasse ab. 
Diese Faktoren werden zusammengefasst durch einen \textit{Algorithmus} $\mathcal{A}$ \cite{shalev}. Damit definiert man $h_S = \mathcal{A}(S)$, wobei die Menge $S$ eine 
Stichprobe der Verteilung $\mathcal{D}$ mit der Gr\"o{\ss}e $n$ darstellt. \\


Um eine Vorhersagefunktion $h_S$ zu finden, welche die wahre Funktion $f$ m\"oglichst gut ann\"ahert, muss definiert werden, was es bedeutet, dass 
$h_S$ \textit{nahe} an $f$ liegt. Die folgenden Definitionen folgen Shalev (2014) \cite{shalev}. 
Um zu beschreiben, wie nahe $h_S$ an $f$ liegt, wird die \textit{loss}-Funktion $\ell$ mit
$$ \ell : \mathcal{H} \times \mathcal{Z} \to \mathbb{R}_+ $$
definiert. $\mathcal{H}$ entspricht der Funktionklasse aus der $h_S$ gew\"ahlt werden soll und $\mathcal{Z} = 
\mathcal{X} \times \mathcal{Y}$, die Menge aller m\"oglichen Daten. Mithilfe von $\ell$ l\"asst sich die \textit{wahre risk}-Funktion $L_{\mathcal{D}}$ mit

$$ L_{\mathcal{D}}(\mathcal{A}) = \mathbb{E}[\ell(\mathcal{A}(S), (X,Y))]$$

mit $S \backsim \mathcal{D}^{\otimes n}$, $\mathcal{A}(S) \in \mathcal{H}$ und $(\mathbf{X},Y) \in \mathcal{Z}$ definieren. Jeder Machine Learning Algorithmus versucht diese Funktion so klein wie m\"oglich zu halten.


Da f\"ur die Bildung von $h_S$ nur die endliche Menge $S\in\mathcal{Z}^n$ an Trainingsdaten zur Verf\"ugung steht wird die 
\textit{empirische risk}-Funktion $L_S$ mit
$$ L_S(h_S) = \frac{1}{n}\sum_{i=1}^n \ell(h_S, (\mathbf{x}_i,y_i))$$
gebildet. Oft ist ein Algorithmus dahingehend gebildet, dass er das empirische Risiko minimiert, mit der Hoffnung, dass damit auch das wahre Risiko klein wird. \\


Es h\"angt nun vom Algorithmus ab wie gut die wahre risk-Funktion approximiert werden kann. Je nach Wahl der Funktionenklasse und des 
Auswahlverfahrens kann man ein Vorwissen \"uber die Problemstellung mit einflie{\ss}en lassen. Wenn Einschr\"ankungen f\"ur die 
Funktionenklasse aufgrund des Vorwissens vorgenommen werden, kann ein systematischer Approximationsfehler, auch \textit{Bias} genannt, entstehen. \\


Einerseits kann man eine sehr allgemeine Funktionenklasse ausw\"ahlen, wodurch $\mathcal{A}(S)$ alle Daten innerhalb $S$ korrekt reproduziert. Somit 
h\"angt die Bildung von $h_S$ stark von der Stichprobe $S$ ab und $h_{S_1}$ und $h_{S_2}$, wobei $S_1, S_2$ disjunkt sind, liefern unterschiedliche Werte f\"ur Inputdaten, 
die in keiner der beiden Stichproben enthalten sind. Zwar kann der Wert der empirischen risk-Funktion klein gehalten werden, aber dennoch liefert die gebildete 
Vorhersagefunktion gro{\ss}e Werte f\"ur die wahre risk-Funktion. Das ist ein Beispiel daf\"ur, dass es mehr ben\"otigt, um eine gute Vorhersagefunktion zu bilden, als 
die empirische risk-Funktion zu minimieren. \\ 


Andererseits kann man die Funktionenklasse zu sehr beschr\"anken, sodass der Bias gro{\ss} ist und dadurch auch die empirische risk-Funktion f\"ur die 
Vorhersagefunktion hohe Werte liefert. Es k\"onnen zwei F\"alle auftreten \cite{strang}: 
\begin{itemize}
  \item Falls f\"ur $h_S$ das empirische Risiko gering ist, jedoch hohe Werte f\"ur das wahre Risiko erzielt werden, spricht man von \textit{Overfitting}.
  \item Wenn f\"ur $h_S$ selbst das empirische Risiko (und somit auch das wahre Risiko) hoch ist, spricht man von \textit{Underfitting}.
\end{itemize} 

Um Overfitting und Underfitting und eine m\"ogliche Herangehensweisen daran n\"aher zu beschreiben, beschr\"ankt sich die Argumentation ab hier auf Regressionsprobleme und somit gilt
$y \in \mathbb{R}$. Es wird zuerst der Zusammenhang zwischen der loss-Funktion und der Verteilung $\mathcal{D}_{Y|X}$ beschrieben. Danach 
wird das wahre Risiko umgeformt, um es intuitiver verstehen zu k\"onnen. Wenn man anschlie{\ss}end f\"ur einen Algorithmus das wahre Risiko berechnen will,
kann man durch diese Umformung zwischen Overfitting und Underfitting unterscheiden. \\


Man legt mit der Wahl von $\epsilon$ eine Klasse von Verteilungen fest, deren Elemente durch Parameter, wie beispielsweise den Erwartungswert oder den Median,  
beschrieben werden. $\epsilon$ soll entsprechend der realen Problemstellung angenommen werden. 
(Beispielsweise hat man bei Messprozessen ein Vorwissen \"uber die Verteilung der Messfehler.) 
Die Vorhersagefunktion $h_S$ approximiert schlussendlich einen 
der Parameter von $\mathcal{D}_{Y|X}$. Welchen Parameter der bedingten Verteilung die Vorhersagefunktion approximieren soll, h\"angt 
mit der angenommenen Form von $\epsilon$ zusammen. Es ist sinnvoll, dass bei normalverteiltem Rauschen 
$h_S$ den bedingten Erwartungswert von $\mathcal{D}_{Y|X}$ approximiert, weil die Normalverteilung mit dem Erwartungswert parametrisiert wird. 
Andererseits soll bei laplaceverteiltem Rauschen $h_S$ den bedingten Median von $\mathcal{D}_{Y|X}$ approximieren, weil die 
Laplaceverteilung mit dem Median parametrisiert wird. \\


Es ist wichtig hervorzuheben, dass die Wahl des entsprechenden Parameters auch mit der Wahl der loss-Funktion zusammenh"angt. 
Im Folgenden wird gezeigt, welche loss-Funktionen f\"ur den bedingten Erwartungswert und bedingten Median gew\"ahlt werden m"ussen.


Wenn f\"ur eine Zufallsvariable $U$ mit einer stetigen Dichte $f_U(u) > 0$ und einem endlichen Erwartungswert die Funktion $g(\phi) = \mathbb{E}[r(U - \phi)]$ minimiert wird, 
kann man mit der Wahl von $r(\cdot)$ 
festlegen, welchen Wert das Minimum annimmt. Es wird nun gezeigt, dass (1) bei der Wahl von $r(U - \phi) = (U - \phi)^2$ das Minimum von $g(\phi)$ an der Stelle 
$\phi = \mathbb{E}[U]$ angenommen wird. Danach wird gezeigt, dass (2) bei der Wahl von $r(U - \phi) = |U - \phi|$ das Minimum von $g(\phi)$ an der Stelle 
$\phi = m(u)$ angenommen wird, wobei $m(u)$ der Median von $U$ ist und durch $\int_{-\infty}^{m(u)} k(u) \, du = \frac{1}{2}$ definiert ist. 

\textit{Beweis zu (1)}: \\
Um die Funktion $g(\phi) = \mathbb{E}[(U - \phi)^2]$ zu minimieren betrachten wir ihre erste und zweite Ableitung.
$$ g'(\phi) = \frac{d}{d\phi}\mathbb{E}[(U-\phi)^2] = \mathbb{E}[-2(U-\phi)] = -2\mathbb{E}[U] + 2\phi$$
Hier darf Differentiation und Erwartungswert vertauscht werden, weil alle Bedingungen des Satzes \textit{Differentiation unter dem Integralzeichen} 
nach Elstrodt (1996) erf\"ullt sind \cite[Kapitel 4, Satz 5.7]{elstrodt}.
Der oben berechnete Ausdruck wird an der Stelle $\phi = \mathbb{E}[U]$ null und ist dadurch ein kritischer Punkt. Dar\"uber hinaus ist die zweite 
Ableitung nach $\phi$ gr\"o{\ss}er null:
$$g''(\phi) = \frac{d}{d\phi}(-2\mathbb{E}(U)+2\phi) = 2 > 0$$
Somit ist die gefunde Stelle ein Minimum. $\square$


\textit{Beweis zu (2)}: \\
Zuerst setzen wir f\"ur den Ausdruck $g(\phi) = \mathbb{E}[|U-\phi|]$ die Definition des Erwartungswertes ein und formen diese um.
\begin{equation*}
\begin{split}
  \mathbb{E}[|U-\phi|]  & = -\int_{-\infty}^{\phi}(u-\phi)f_U(u)  \,du + \int_{\phi}^{\infty}(u-\phi)f_U(u)  \,du \\\
 & = - \int_{-\infty}^{\phi}uf_U(u)  \,du + \phi \int_{-\infty}^{\phi}f_U(u)  \,du + \int_{\phi}^{\infty}uf_U(u)  \,du -\phi \int_{\phi}^{\infty}f_U(u)  \,du \\\
 & = - \int_{-\infty}^{\phi}uf_U(u) \,du + \phi \int_{-\infty}^{\phi}f_U(u) \,du + \mathbb{E}[U] \\\
 & \quad - \int_{-\infty}^{\phi}uf_U(u) \,du - \phi \bigl(1 - \int_{-\infty}^{\phi}f_U(u) \,du \bigr)
\end{split}
\end{equation*}

Nun leiten wir diesen Ausdruck nach $\phi$ ab und setzen in gleich null:
\begin{equation*}
  \begin{split}
    g'(\phi) & = \frac{d}{d\phi}g(\phi) = -\phi f_U(\phi) + \int_{-\infty}^{\phi}f_U(u) \,du + \phi f_U(\phi) \\\
    & \quad - \phi f_U(\phi) - 1 + \phi f_U(\phi) + \int_{-\infty}^{\phi}f_U(u) \,du \\\
    & = 2\int_{-\infty}^{\phi}f_U(u) \,du - 1 \overset{!}{=} 0 \\\
    & \Leftrightarrow \int_{-\infty}^{\phi}f_U(u) \,du = \frac{1}{2}
  \end{split}
\end{equation*}
Dieser Ausdruck entspricht der Definition des Median von $U$ und einen kritischen Punkt von $\phi \mapsto \mathbb{E}[|U - \phi|]$. Weil die zweite Ableitung 
$$ g''(\phi) = f_U(\phi) > 0 $$
die Dichte von $U$ ist, die immer positiv ist, handelt es sich um ein Minimum. $\square$



Im folgenden Abschnitt versucht man das wahre Risiko besser zu verstehen. Es wird der Argumentation von Bishop (2006) gefolgt \cite{bishop}.
Daf\"ur ist es notwendig einerseits das den \textit{erwarteten Output} $\bar{y}$ zu definieren:

$$ \bar{y}(\mathbf{x}) = \mathbb{E}[Y|\mathbf{X}=\mathbf{x}] = \int_y y f_{Y|\mathbf{X}}(y|\mathbf{x}) \, dy.$$

Andererseits ben\"otigt man die \textit{erwartete Vorhersagefunktion} $\bar{h}$, welche, vorausgesetzt eines Algorithmus, gegeben ist mit:
\begin{equation*}
  \begin{split}
    \bar{h} & = \mathbb{E}[\mathcal{A}(S)] = \int_{\mathbb{R}^{(d+1)n}} h_s p_S(s)  \,ds = \\\
    & \quad \int_{\mathcal{Z}^n}\mathcal{A}(\{ (x_1, y_1), \dots ,(x_n, y_n)\}) \prod_{k=1}^n f_{X,Y}(x_k, y_k) \, dx_1 dy_1 \dots dx_n dy_n.
  \end{split}
\end{equation*}


Den erwarteten Output kann man verstehen als Erwartungswert aller m\"oglichen $y$, gegeben ein festes $\mathbf{x}$. 
Um die erwartete Vorhersagefunktion zu bilden, geht man davon aus, dass 
die Funktion $h_S$ von der zuf\"alligen Stichprobe $S$ abh\"angig ist. Man kann sich vorstellen, dass man unendliche viele Stichproben $S$ zieht und 
f\"ur jedes $S$ bekommt man eine andere Funktion $h_S$. Danach wird auf ein $\mathbf{x}$ jede dieser unterschiedlichen Funktionen $h_S$ angewendet und 
anschlie{\ss}end gemittelt. \\


Ab hier beschr\"anken wir uns auf die loss-Funktion $ \ell(h, (\mathbf{x},y)) = (h(\mathbf{x}) - y)^2 $, weil dadurch die Argumentation vereinfacht wird. 
Das bedeutet $h_S$ approximiert den bedingten Erwartungswert von 
$Y$ gegeben $\mathbf{X}$, wie oben gezeigt wurde. Nun wollen wir die wahre risk-Funktion von $\mathcal{A}$ betrachten:

$$ \mathcal{L}_{\mathcal{D}}(\mathcal{A}) = \mathbb{E}[(h_S(\mathbf{\mathbf{X}})-Y)^2)],$$
wobei gilt, dass $(\mathbf{X}, Y) \backsim \mathcal{D}$ und $S \backsim \mathcal{D}^{\otimes n}$.
Das Ziel ist es, diesen erwarteten Fehler so umzuformen, dass wir ihn in Teile zerlegen k\"onnen, die verst\"andlicher sind. 
\begin{equation*}
  \begin{split}
    \lefteqn{ \mathbb{E}[(h_S(\mathbf{X})-Y)^2)] } \\\
    & = \mathbb{E}[\bigl((h_S(\mathbf{X})-\bar{h}(\mathbf{X}))+(\bar{h}(\mathbf{X})-Y)\bigr)^2] \\\
    & = \mathbb{E}[(h_S(\mathbf{X})- \bar{h}(\mathbf{X}))^2] + \mathbb{E}[(\bar{h}(\mathbf{X})-Y)^2] + 2\underbrace{\mathbb{E}[(h_S(\mathbf{X})-\bar{h}(\mathbf{X}))(\bar{h}(\mathbf{X})-Y)]}_{=0} \\\
    & = \mathbb{E}[(h_S(\mathbf{X})- \bar{h}(\mathbf{X}))^2] + \mathbb{E}[(\bar{h}(\mathbf{X})-Y)^2] \\\
    & = \mathbb{E}[(h_S(\mathbf{X})- \bar{h}(\mathbf{X}))^2] + \mathbb{E}[\bigl( (\bar{h}(\mathbf{X})-\bar{y}(\mathbf{X}))+(\bar{y}(\mathbf{X})-Y)\bigr)^2 ] \\\
    & = \mathbb{E}[(h_S(\mathbf{X})- \bar{h}(\mathbf{X}))^2] + \mathbb{E}[(\bar{h}(\mathbf{X})-\bar{y}(\mathbf{X}))^2] +\mathbb{E}[(\bar{y}(\mathbf{X})-Y)^2] \\\
    &+ 2\underbrace{\mathbb{E}[(\bar{h}(\mathbf{X})-\bar{y}(\mathbf{X}))(\bar{y}(\mathbf{X})-y)]}_{=0} \\\
    & = \underbrace{\mathbb{E}[(h_S(\mathbf{X})- \bar{h}(\mathbf{X}))^2]}_{\text{Variance}}+\underbrace{\mathbb{E}[(\bar{h}(\mathbf{X})-\bar{y}(\mathbf{X}))^2]}_{\text{Bias}^2} + \underbrace{\mathbb{E}[(\bar{y}(\mathbf{X})-Y)^2]}_{\text{Noise}}
  \end{split}
\end{equation*}

Der letzte Summand in Zeile 3 ist null, weil:
\begin{equation*}
  \begin{split}
    \lefteqn{ \mathbb{E}[(h_S(\mathbf{X})-\bar{h}(\mathbf{X}))(\bar{h}(\mathbf{X})-Y)] } \\\
    & = \mathbb{E}[\mathbb{E}[h_S(\mathbf{X})-\bar{h}(\mathbf{X})](\bar{h}(\mathbf{X})-Y)] \\\
    & = \mathbb{E}[(\mathbb{E}[h_S(\mathbf{X})]-\bar{h}(\mathbf{x}))(\bar{h}(\mathbf{x})-Y)] \\\
    & = \mathbb{E}[(\bar{h}(\mathbf{X})-\bar{h}(\mathbf{X}))(\bar{h}(\mathbf{X})-Y)] \\\
    & = \mathbb{E}[0]\\\
    & = 0
  \end{split}
\end{equation*}


Weiters ist der letzte Summand in Zeile 7 null, weil:
\begin{equation*}
  \begin{split}
    \lefteqn{ \mathbb{E}_{X, Y}[(\bar{h}(\mathbf{X})-\bar{y}(\mathbf{X}))(\bar{y}(\mathbf{X})-Y)] } \\\
    & = \mathbb{E}[\mathbb{E}[\bar{y}(\mathbf{X})-Y|\mathbf{X} = \mathbf{x}](\bar{h}(\mathbf{X})-\bar{y}(\mathbf{X}))] \\\
    & = \mathbb{E}[(\bar{y}(\mathbf{X})-\mathbb{E}[y|\mathbf{X} = \mathbf{x}])(\bar{h}(\mathbf{x})-\bar{y}(\mathbf{x}))] \\\
    & = \mathbb{E}[(\bar{y}(\mathbf{X})-\bar{y}(\mathbf{X}))(\bar{h}(\mathbf{X})-\bar{y}(\mathbf{X}))]] \\\
    & = \mathbb{E}[0]\\\
    & = 0
  \end{split}
\end{equation*}


Diese drei verbliebenen Terme sind:
\begin{itemize}
  \item Die Varianz der Vorhersagefunktion an sich. Je nach Ziehung der Stichprobe $S$ kann $h_S$ unterschiedlich sein.
  \item Der Bias$^2$ ist die systematische Abweichung der Vorhersagefunktion. %Quadrat erkl\"aren ist zu kompliziert!
  \item Der Noise gibt an wie schwer die Aufgabe an sich ist. Er gibt die Varianz von $\epsilon$ an. Der Noise ist die untere Grenze des wahren Risikos.
\end{itemize}

Somit besagt die Gleichung:
$$ \text{Risk} = \text{Varianz} + \text{Bias}^2 + \text{Noise} $$
und das wahre Risiko kann in drei verst\"andliche Teile eingeteilt werden. Da alle drei Teile keine linearen Funktionen sind, 
versucht man die \textit{Hyperparameter} so zu w\"ahlen, dass das wahre Risiko so klein wie 
m\"oglich wird. Das ist jener Bereich an Komplexit\"at der Funktionenklasse, der zwischen Overfitting und Underfitting liegt. \\


Bei Machine Learning Algorithmen und Modellen versucht man neben der Optimierung der Parameter des entsprechenden Modells 
auch die Hyperparameter der Modellarchitektur zu optimieren. Der entscheidende Unterschied zwischen Hyperparametern
und Parametern ist, dass die Hyperparameter vor dem Training des Modells festgelegt werde m\"ussen und die Parameter w\"ahrend des Trainings
optimiert werden \cite{hyper}. \\

Formal legen Hyperparamter eine Klasse von Funktionen \textit{innerhalb} der zuvor festgelegten Funktionenklasse $\mathcal{H}$ eines Machine 
Learning Algorithmus fest. Beispielsweise wird festgelegt, ob man bei einer linearen Regression eine Linearkombination der Inputvariablen, oder auch deren Quadrate 
zul\"asst. Diese Entscheidung muss vor dem Training getroffen werden. \\


Sowohl bei einer Regressions- als auch bei einer Klassifikationsaufgabe ist das Ziel der jeweiligen Prediktorfunktion $h_S$ eines Machine Learning Algorithmus, 
ein Muster in den Daten zu finden, welches f\"ur Menschen nicht erkennbar w\"are. Oft ist es f\"ur Menschen schwierig die Vorgehensweise des trainierten Algorithmus 
nachzuvollziehen. \\

Nun werden jene Machine Learning Algorithmen kurz beschrieben, welche in der vorliegenden Problemstellung zum Einsatz gekommen sind.





























\subsection{Multiple Lineare Regression}

Hier wird versucht die bedingte Verteilung von $\mathcal{D}_{Y|X}$ mit einer bestimmten Klasse an Vorhersagefunktion bestm\"oglich zu beschreiben. 
Wiederum folgen $\mathbf{X}$ und $Y$ einer gemeinsamen Verteilung $\mathcal{D}$, wobei man $Y$ mit $Y = f(\mathbf{X}) + \epsilon$ darstellen kann. 
Ab hier beschränken wir uns darauf, dass $\mathbf{X}$ nur eindimensional ist, um die Notation zu vereinfachen. \\

Es gibt viele M\"oglichkeiten um die Verteilung von $Y$ zu beschreiben. Die einfachste Wahl w\"are es, $\mathbb{E}[Y]$ zu verwenden. Nur so verwendet man nicht die 
Information, die man durch ein Realisierung von $X$ erh\"alt. Eine bessere L\"osung ist es, den bedingten Erwartungswert $\mathbb{E}[Y|X]$ zu verwenden. Somit ben\"utzt man auch die Information aus der 
Realisierung von $X$. Man kann beispielsweise auch, wie oben gezeigt, den bedingten Median verwenden. Diese Entscheidung sollte von der vorliegenden Problemstellung, 
abh\"angen. Am h\"aufigsten nimmt man den bedingten Erwartungswert \cite{bishop}. Die Funktion 
$$ x \mapsto f(x) := \mathbb{E}[Y|X = x] = \int y \cdot p_{Y|X}(y|x) \,dy $$
wird als Regressionsfunktion bezeichnet \cite{wasserman}. Im folgenden wird nun versucht, die Funktion $f$ m\"oglichst gut zu sch\"atzen.\\

Nun gibt es viele M\"oglichkeiten die Funktionklasse f\"ur den Prediktor $h_S$ zu w\"ahlen. Bei der linearen
Regression nimmt an, dass die Funktion linear in wenigen Parametern ist. Beispiele sind:
$$ h_S(x) = a + bx $$ 
oder 
$$ h_S(x) = a + bx + cx^2$$
wobei beide zur linearen Regression geh\"oren. Man sieht, dass die Funktion eine Linearkombination von sogenannten \textit{Basic Functions}, und somit 
linear in ihren Parametern ist \cite{bishop}. (Von nun an wird die Argumentation am einfachsten Beispiel von $ h_S(x) = a + bx $ fortgef\"uhrt.)\\ 

Die Klasse von Funktionen aus der ein passender Sch\"atzer $h_S$ gefunden werden kann wird durch diese Annahme sehr eingeschr\"ankt. Das f\"uhrt zu einer
Reduktion der Varianz, was m\"oglichem overfitting entgegenwirkt. Es kann aber sein, dass die Funktion aus dieser Klasse f\"ur die Problemstellung zu simpel ist, und 
man dadurch die wahre Funktion $f$ nicht genau genug beschreiben kann. \\

Nun kann man einen Sch\"atzer f\"ur den bedingten Erwartungswert bilden.
Weil der Erwartungswert von $X$ die Funktion $g(\phi) = \mathbb{E}[(X - \phi)^2]$ minimiert, kann man den \textit{Least Squares Sch\"atzer} wie folgt definieren:
$$ (\hat{a}, \hat{b}) = \argmin_{(a,b) \in \mathbb{R}^2} \sum_{i=1}^n (Y_i - a - bX_i)^2$$
wobei $S = \{(X_1, Y_1), \dots , (X_n,Y_n)\}$.

Wie oben gezeigt, berechnet man mit dem Least Squares Sch\"atzer den bedingten Erwartungswert von $y$, gegeben $x$. 
Wenn die Summe der absoluten Abst\"ande minimiert, bekommt man einen Sch\"atzer f\"ur den bedingten Median. Man verwendet die Tatsache, dass
der Median die Funktion $g(\phi) = \mathbb{E}[|X - \phi|]$ minimiert. Hierfür müsste aber die Regressionsfunktion anders definiert sein. \\ 

Es ist hilfreich, dass es L\"osungen in geschlossener Form gibt, die $\sum_{i=1}^n (Y_i - a - bX_i)^2 $ minimieren. Somit kann man die Sch\"atzer leicht berechnen.
Weiters ist es wie oben erw\"ahnt m\"oglich, die abh\"angige Variable $Y$ auch von mehreren unabh\"angigen Variablen beschreiben zu lassen, indem $\mathbf{X}$ einen Zufallsvektor beschreibt.
Dadurch erh\"alt man die \textit{Multiple Lineare Regression} 
mit der Form 
$$ Y = \mathbf{X}^T\mathbf{\beta} + c. $$
Die Vorgehensweise um einen Sch\"atzer zu finden, ist analog zu der im eindimensionalen Fall, indem man die Rechenregeln f\"ur Vektoren beachtet. \\ 

Im Falle des mehrdimensionalen Modells ist die Annahme, dass die \glqq abh\"angige\grqq{} Variable $Y$ eine Linearkombination der \glqq unabh\"angigen\grqq{} Variablen
(Eintr\"age von $\mathbf{X}$), oder \textit{Basic Functions} von ihnen ist. Diese Annahme ist sehr einschr\"ankend, da man im Vorhinein wenig \"uber die gegenseitige Beeinflussung der 
\glqq unab\"angigen\grqq{} Variablen untereinander aussagen kann. \\

Die L\"osungen f\"ur den eindimensionalen Fall sind \cite{wasserman}:

$$ \hat{b} = \frac{\overline{XY} - \bar{X}\bar{Y}}{\overline{X^2} - (\bar{X})^2} $$
$$ \hat{a} = \bar{Y} - \hat{b}\bar{X} $$
mit $\bar{X} = \frac{1}{n} \sum_{i = 1}^n X_i$, $\bar{Y} = \frac{1}{n} \sum_{i = 1}^n Y_i$, $\overline{X^2} = \frac{1}{n} \sum_{i = 1}^n X_i^2$ und $\overline{XY} = \frac{1}{n} \sum_{i = 1}^n X_iY_i$.

Die L\"osungen f\"ur den mehrdimensionalen Fall sind \cite{wasserman}: 

$$  \hat{\beta} = (\mathbb{X}^T\mathbb{X})^{-1}\mathbb{X}^T\mathbf{Y} .$$
Wobei $\mathbb{X} = [\mathbf{x}_1, \dots, \mathbf{x}_n]^T$. Man kann die letzte Gleichung wie folgt umformen:

$$ \mathbb{X}\hat{\beta} = P\mathbf{Y}, P = \mathbb{X}(\mathbb{X}^T\mathbb{X})^{-1}\mathbb{X}^T $$

Somit erh\"alt man eine geometrische Interpretation der Regression. $\mathbb{X}\hat{\beta}$ stellt die Orthogonalprojektion des Punktes $\mathbf{Y}$ auf jene
Hyperebene dar, die von den unterschiedlichen Dimensionen in $\mathbb{X}$ aufgespannt wird. \\

Bei der multiplen linearen Regression kann man auch eine Hyperparameterauswahl treffen. Es muss \"uberlegt werden, welche 
\textit{Basic Functions} man ausw\"ahlt. 






























\subsection{Logistische Regression}
Bei der logistischen Regression wird angenommen, dass $Y \in \{0,1\}$ und somit Bernoulli verteilt ist.
Dadurch ist sie ein Klassifizierungsmodell. ($\mathbf{X}$ kann auch vektorwertig sein.)  \\

Somit ist auch $Y$ gegeben $\mathbf{X}$ eine Bernoulli Zufallsvariable mit ihrem Erwartungswert $\mu(x) = \mathbb{E}[Y|\mathbf{X} = \mathbf{x}] \in (0,1)$. Man kann nicht die Annahme treffen, dass 
$\mu(\mathbf{x}) = \mathbf{x}^T\beta$, weil diese Funktion nach $\mathbb{R}$ abbilden w\"urde. Bei der logistischen Regression ben\"utzt man eine invertierbare Funktion $\gamma$ mit 
$\gamma(\mathbf{x}^T\beta) = \mu(\mathbf{x}) \in (0,1)$. \\

Somit ist die Funktionenklasse, aus welcher ein Prediktor $h_S$ gew\"ahlt wird, nicht mehr auf $h_S(\mathbf{x}) = \mathbf{x}^T\beta$ beschr\"ankt, 
sondern wird auf  $h_S(\mathbf{x}) = \gamma(\mathbf{x}^T\beta)$ erweitert. Die Umkehrfunktion $\gamma^{-1}$ wird auch mit $\alpha(\cdot)$ bezeichnet, 
und \textit{Link Function} genannt \cite{wasserman}. Durch diese Generalisierung muss die Regressionsfunktion nicht zwingend linear in den Basic Functions sein. 
Nur bevor die sogenannte Link Funktion auf sie angewendet wurde, ist sie linear in ihren Basic Functions. \\ 

Im Falle der Bernoulli Verteilung, welche bei der logistischen Regression als Grundannahme gilt, ist die Link Funktion mit:
$$ \alpha(x) = \gamma^{-1}(x) = \log(\frac{x}{1-x}) = \log(x) - \log(1-x) $$
gegeben und wird als \textit{logit-link} bezeichnet. Die Funktion $\gamma(x) = \frac{e^x}{1 + e^x}$ wird als \textit{logistische Funktion} bezeichnet \cite{wasserman}. 
Daher kommt auch der Name \glqq logistische Regression\grqq{}. \\

% In diesem Fall kann man die passende Link Funktion finden, da die Bernoulli Verteilung zu der Familie der Exponentialverteilungen geh\"ort und es einen
% einheitliche Vorgang f\"ur das Auffinden der Link Funktionen aus dieser Familie gibt. Generell muss das aber nicht der Fall sein. \\

Man kann nun das Modell der logistischen Regression wie folgt formulieren: \\
Seien $(\mathbf{X}_i, Y_i) \in \mathbb{R}^m \times \mathbb{R}$, $i = 1, \dots , n$  unabh\"angige Paare von Zufallsvariablen, sodass die bedingte Verteilung von 
$Y_i$ gegeben $X_i = x_i$ die Wahrscheinlichkeitsfunktion 

$$ f_{\mathbf{x}_i^T\beta}(y_i) = \exp( y_i(\mathbf{x}_i^T\beta) - \log(1 + e^{(\mathbf{x}_i^T\beta)})) = \frac{e^{y_i(\mathbf{x}_i^T\beta)}}{1 + e^{(\mathbf{x}_i^T\beta)}}$$

besitzt \cite{wasserman}. (Man hat hier die Tatsache verwendet, dass die Bernoulli Verteilung zur Familie der Exponentialverteilungen geh\"ohrt.) \\ 

Wenn man durch Konstruktion des Maximum Liklihood Sch\"atzers, den Parameter $\beta$ mit $\hat{\beta}$ absch\"atzen kann, ist es wiederum m\"oglich mittels der Funktion $\gamma(\mathbf{x}^T\hat{\beta})$
den gesch\"atzten Parameter $\hat{p}$ der Bernoulli Verteilung von $Y$ gegeben $\mathbf{X}$ zu bilden. Nun hat man eine Wahrscheinlichkeit mit der die abh\"angige Variable $Y$ gegeben $\mathbf{X}$ den Wert 1 annimmt.
Um schlussendlich eine Klassifikation durchzuf\"uhren setzt man manuell einen Schwellwert, anhand diesem entschieden werden kann, ob man die Variable 1 oder 0 gesch\"atzt werden soll. 
























\subsection{Random Forest Modelle}
Random Forest Modelle sind eine Form des \textit{Ensemble Learnings} und wurden erstmals im Jahr 2001 von Breiman verwendet \cite{breiman2001random}. 
Die individuellen Prediktorfunktionen eines Random 
Forest Modells nennt man \textit{Decision Trees}.
Ensemble Learning besteht darin, dass die Outputs mehrerer Prediktorfunktionen zu einem Output zusammengefasst werden. Der Wert dieses Outputs entspricht
dem Wert, den die Mehrheit der individuellen Prediktorfunktionen angenommen hat.
Man kann diese Vorgehensweise mit einer Abstimmung beziehungsweise der Mehrheitsmeinung vergleichen \cite{handson}. \\

Die Idee dabei ist, dass viele Klassifikatoren, die individuell kaum besser sind als eine Zufallsmeinung, in der Gruppe einen 
besseren Klassifikator darstellen. (Man kann dieses Ph\"anomen mit einer M\"unze vergleichen, die zu $51\%$ mit Kopf nach oben liegen bleibt. Wirft 
man sie ein paar mal, wird man nicht wirklich erkennen k\"onnen, welche Seite favorisiert wird. Wenn man sie aber sehr oft wirft, bekommt man 
ein immer stabilers Ergebnis dar\"uber, welche Seite favorisiert wird.) \\

Die Zusammenf\"uhrung mehrerer Decision Trees beugt auch Overfitting einzelner Prediktorfunktionen vor \cite{shalev}. Weiters 
werden bei dem verwendeten Random Forest Modell Vorgaben an die einzelnen 
Decision Trees gegeben. Beispielsweise werden die individuellen Decision Trees nur auf einer Teilmenge der Trainingsdaten trainiert. Hyperparameter f\"ur das Random Forest Modell sind 
die Anzahl der Decision Trees, die Anzahl der Trainigsdaten pro Decision Tree und auch die spezifischen Hyperparameter der Decision Trees an sich. In \hyperref[fig:rf1]{Abbildung 2.2} ist der schematische Aufbau 
eines stark vereinfachten Decision Trees dargestellt. \\

\begin{figure}[ht]
  \label{fig:rf1}
  \begin{center}
    \begin{tiny}
    \tikzfig{fig3}
    \end{tiny}
  \end{center}
  \caption[Schematische Darstellung eines Decision Tree Modells]
    {An jedem Knoten wird nach einem Kriterium bestm\"oglich entschieden. Sollte eine gewisse Tiefe erreicht sein, 
    oder es nur mehr eine Klasse geben, ist man \textit{Blatt} des Decision Trees angelangt.}
\end{figure}



Nun wird das mathematische Modell eines Decision Trees genauer erkl\"art. (Random Forest Modelle beruhen auf der Verwendung mehrerer, verschiedener Decision Trees.)
Grunds\"atzlich stellen sich bei einem Decision Tree folgende Fragen \cite{shalev}:
\begin{itemize}
  \item Nach welcher Inputvariable soll unterteilt werden?
  \item Wo soll die Unterteilung der gew\"ahlten Variable stattfinden?
\end{itemize}

Um diese Fragen zu beantworten wird im Folgenden das verwendete Decision Tree Modell mathematisch beschrieben. Die Argumentation folgt jener der Implementierung von \textit{Scikit-Learn} \cite{sklearn}.
Gegeben der Trainingsvektoren $\mathbf{x}_i \in \mathbb{R}^m$, $i=1,\dots ,n$ und die entsprechenden Zielwerten $y_i$, geht ein Decision Tree rekursiv vor, um den 
Raum der Inputvariablen zu unterteilen und nach den Zielwerten zu gruppieren. Die Daten am Entscheidungsknoten $v$ werden mit 
$Q_v$ bezeichnet, wobei $Q_v$ die Menge von Trainingsdaten $\{(\mathbf{x}_1, y_1, \dots , (\mathbf{x}_{N_v}, y_{N_v})\}$ darstellt, welche an diesem 
Entscheidungsknoten noch \"ubrig geblieben ist. Die $N_v$ \"ubrig 
gebliebenen Daten werden nach jedem m\"oglichen 
Split $\theta = (j, t_v)$, bestehend aus Inputvariablen $j$ und dem Schwellwert $t_v$, in $Q_v^{\text{left}}(\theta)$ und $Q_v^{\text{right}}(\theta)$ unterteilt. Dabei gilt:
$$ Q_v^{\text{left}}(\theta) = \{ (\mathbf{x},y) | \mathbf{x}^{(j)} \leq t_v \} $$
$$ Q_v^{\text{right}}(\theta) = Q_v \backslash Q_v^{\text{left}}(\theta) $$
Es wird unter allen Kandidaten $\theta$ mittels einer loss-Funktion $\ell(\cdot)$, die sich je nach Klassifizierungs- oder Regressionsaufgabe unterscheidet, entschieden
welche Wahl am besten ist:
$$ G(Q_v, \theta) = \frac{N_v^{\text{left}}}{N_v}\ell(Q_v^{\text{left}}(\theta)) + \frac{N_v^{\text{right}}}{N_v}\ell(Q_v^{\text{right}}(\theta)) $$ 
Der Kandidat $\theta^* = \argmin_{\theta}G(Q_v, \theta)$ wird ausgew\"ahlt. Danach wird rekursiv vorgegegangen bis die maximale Tiefe des Decision Trees, 
$N_v < \operatorname{min}_{samples}$ oder $N_v = 1$ erreicht wurde. Hierzu werden Greedy-Algorithmen wie beispielsweise ID3 \cite{ID3} oder CART \cite{breiman1984classification} verwendet.\\

Bei Klassifizierungsaufgaben wird f\"ur $\ell$ die \textit{Entropy} verwendet. Bei Regressionsaufgaben wird entweder der \textit{Mean Squared Error} oder 
der \textit{Mean Absolute Error} verwendet. \\

Sei $p_{vk} = 1/N_v \sum_{y \in Q_v} \mathbbm{1}_{y = k}$ 
die relative H\"aufigkeit der Klasse $k$ am Entscheidungsknoten $v$, $\operatorname{mean}_v(y) = \frac{1}{N_m}\sum_{y\in Q_m}y$ und $ \median_v(y) = \median_{y \in Q_v}(y)$, dann sind: \\


Mean Squared Error:  $$ \ell(Q_v) = \frac{1}{N_v} \sum_{y\in Q_v} (y - \operatorname{mean}_v(y))^2 $$
Mean Absolute Error: $$ \ell(Q_v) = \frac{1}{N_v} \sum_{y\in Q_v} |y - \median_v(y)| $$
Entropy: $$ \ell(Q_v) = - \sum_{k} p_{vk}\log(p_{vk}) $$


Wichtig hervorzuheben ist, dass es m\"oglich ist mehrmals nach einer Inputvariablen an unterschiedlichen Punkten aufzuteilen. 
Dadurch kann man eine Teilung in mehrere Gruppen durch mehrere bin\"are Teilungen ersetzen.
Dar\"uber hinaus ist der Decision Tree mit einer beliebigen Tiefe $w$ in dem Decision Tree der Tiefe $w + 1$ enthalten. \\



In \hyperref[fig:rf2]{Abbildung 2.3} ist dargestellt wie eine Regression eines Decision Trees aussehen kann. Weiters sieht man, wie eine m\"ogliche Unterteilung des 
Raumes der Inputvariablen durchgef\"uhrt werden kann. Bei einer Klassifikation eines Decision Trees kann man sich auch an jedem Endknoten nicht nur die entsprechende Klasse 
ausgeben lassen, sondern auch deren Wahrscheinlichkeit. Diese ist einfach die relative H\"aufigkeit der Outputklasse in der Teilmenge der \"ubriggebliebenen Trainingsbeispiele \cite{sklearn}.  \\ 

\begin{figure}[ht]
  \label{fig:rf2}
  \begin{center}
    \begin{tiny}
    \tikzfig{fig4}
    \end{tiny}
  \end{center}
  \caption[Regression eines Random Forest Modells]
    {Links sieht man eine fiktive Regression der Daten. Rechts ist eine schematische Unterteilung eines zweidimensionalen Inputraumes dargestellt.}
\end{figure}





























\subsection{Support Vector Machine Modelle}

Die Modellarchitektur von Support Vector Machine Modellen wurde von Vladimir Vapnik \cite{SVMVapnik} in den Neunizigerjahren eingef\"uhrt. Man kann das 
Support Vector Machine Modell f\"ur Klassifizierungs- und auch f\"ur Regressionsprobleme verwenden. Hier wird die Funktionsweise des Klassifizierungsmodells
beschrieben. Wir betrachten in diesem Abschnitt nur Klassifikatoren. \\

Bei diesem Modell wird ausschlie{\ss}lich versucht die Beispieldaten nach ihren Eigenschaften linear zu separieren. Das bedeutet, man versucht f\"ur die 
Daten $S = (\mathbf{x}_1,y_1),\dots, (\mathbf{x}_m, y_m)$ mit $\mathbf{x_i} \in \mathbb{R}^n$ und $y_i \in \{\pm1\}$ einen Halbraum $(\mathbf{w},b)$ zu finden 
mit $y_i = \operatorname{sign}(\left\langle \mathbf{w},\mathbf{x}_i\right\rangle + b), \forall i$. Dadurch treten zwei Probleme auf. Erstens, falls die Daten linear separierbar sind, 
gibt es sehr viele M\"oglichkeiten diesen Halbraum auszuw\"ahlen. Zweitens, falls die Daten nicht linear separierbar sind, versucht man das durch die Berechnung 
weiterer Eigenschaften aus den bisher vorhandenen zu schaffen, wodurch der Rechenaufwand unbew\"altigbar werden kann. Die Modellarchitektur der Support Vector Machines versucht
diese beiden Probleme zu l\"osen \cite{shalev}. \\

Die folgende Erl\"auterung folgt jener von Busuttil \cite{SVM1}. Wenn die Daten linear separierbar sind gibt es nicht nur eine Hyperebene die dies erreicht.
Um eine einzige zu definieren, die auch auf neue Daten gut generalisiert, w\"ahlt man jene Hyperebene aus, die die beiden Klassen mit dem gr\"o{\ss}ten Margin 
voneinander trennt. Diese Aufgabe ist ein convexes Optimierungsproblem mit einer eindeutigen L\"osung. Diese L\"osung beinhaltet nur eine Teilmenge der Trainigsbeispiele, die 
an der Grenze zur anderen Klasse liegen. Diese Beispiele werden als \textit{Support Vectors} bezeichnet. Wie man in \hyperref[fig:svm2]{Abbildung 2.4} sieht, beinhalten die Support Vectoren
alle Information f\"ur eine weitere Klassifizierung und diese ver\"andert sich nicht, wenn man andere Trainingsbeispiele nicht verwendet. \\

\begin{figure}[ht]
  \label{fig:svm1}
  \begin{center}
    \begin{small}
    \tikzfig{fig6}
    \end{small}
  \end{center}
  \caption[Darstellung der Support Vektoren]
    {Die Support Vektoren beinhalten die gesamte Information.}
\end{figure}



Es ist m\"oglich dieses Optimierungsproblem und die Entscheidungsform in \textit{dualer} Form darzustellen \cite{handson}. Die Entscheidungsform hat dadurch folgende Form:
$$ f(\mathbf{x^*}) = \operatorname{sign}(\sum_{i=1}^m y_i\alpha_i \left\langle \mathbf{x^*}, \mathbf{x}_i \right\rangle + b), $$
wobei $\alpha_i \in \mathbb{R}$ als Ma{\ss} an Information von $\mathbf{x}_i$ gesehen werden kann. Sollte $\mathbf{x}_i$ kein Support Vector sein, dann gilt $\alpha_i = 0$.\\

Man unterteilt Support Vector Machine Klassifikatoren in \textit{Hard-Margin} und \textit{Soft-Margin} Klassifikatoren \cite{shalev}. Diese zwei Klassen 
unterscheiden sich darin, dass Hard-Margin Klassifikatoren eine strenge Unterteilung zwischen $y = \pm1$ durchf\"uhren, wohingegen Soft-Margin 
Klassifikatoren eine bestimmte Anzahl an Regelverletzungen zulassen. In den implementierten Modellen werden Soft-Margin Modelle verwendet, und der 
Hyperparameter \textit{C} gibt an, wie viele Regelverletzungen zugelassen werden. Ein niedriger Wert f\"ur C bedeutet mehr Regelverst\"o{\ss}e.  \\

Um zu erreichen, dass die Daten einer Problemstellung linear separierbar sind, werden oft neue Eigenschaften berechnet. Das funktioniert durch die 
Anwendung einer Funktion $\Theta(\cdot)$ auf die vorhandenen Eigenschaften $\mathbf{x}_i$. Das Bild dieser Funktion wird als \textit{Feature Space} (Eigenschaftsraum) bezeichnet.
Es gibt mehrere M\"oglichkeiten diese Funktion zu w\"ahlen. In 
\hyperref[fig:svm2]{Abbildung 2.5} sieht man, dass durch die Anwendung von $\Theta(x) = x^2$ die Daten linear separierbar gemacht wurden. \\

\begin{figure}[ht]
  \label{fig:svm2}
  \begin{center}
    \begin{tiny}
    \tikzfig{fig5}
    \end{tiny}
  \end{center}
  \caption[Berechnung zus\"atzlicher Features aus vorhandenen Inputs]
    {Durch die Berechnung zus\"atzlicher \textit{Features} wurden die Daten linear separierbar.}
\end{figure}

Die Entscheidungsfunktion $f$ ben\"utzt das Innere Produkt von Vektoren aus dem Eigenschaftenraum. Durch die Funktion $\Theta$ in den
Feature Space bedeutet, dass man nun $\left\langle \Theta(\mathbf{x}), \Theta(\mathbf{y}) \right\rangle$ verwendet. Das kann einen sehr hohen Rechenaufwand 
mit sich ziehen. Jedoch kann durch die Anwendung des \textit{Kernel Tricks} der Rechenaufwand in Grenzen gehalten werden. Ein Kernel ist eine Funktion 
der Form $k(\mathbf{x}, \mathbf{y}) = \left\langle \Theta(\mathbf{x}), \Theta(\mathbf{y}) \right\rangle$. Laut dem \textit{Mercer Theorem} ist eine Funktion 
$k$, die gewisse Eigenschaften (Mercer Bedingungen) erf\"ullt ein Kernel zu einer zugeh\"origen Funktion $\Theta(\cdot)$. Um 
diesen Kernel zu verwenden, muss man die Funktion $\Theta(\cdot)$ nie verwenden und nicht einmal kennen \cite{handson}. \\

Es gibt verschieden Kernels, und die Auswahl h\"angt von der speziellen Aufgabe ab \cite{SVM1}. Im angef\"uhrten Beispiel wird $ \mathbf{x} = [x_1, x_2]^T$ und $\mathbf{y} = [y_1, y_2]^T$ angenommen.
Ein m\"oglicher Kernel ist der Polynomial Kernel mit der Form 
$k(\mathbf{x}, \mathbf{y}) = \left\langle \mathbf{x}, \mathbf{y} \right\rangle^d$. F\"ur $d=2$ gilt:
$$ \left\langle \Theta(\mathbf{x}), \Theta(\mathbf{y}) \right\rangle = \left\langle \mathbf{x}, \mathbf{y} \right\rangle^2, $$
wobei $\Theta(\mathbf{x}) = (x_1^2, \sqrt{2}x_1x_2, x_2^2)^T$ \cite{handson}. 
Somit ist die Funktion $k(\mathbf{x}, \mathbf{y}) = \left\langle \Theta(\mathbf{x}), \Theta(\mathbf{y}) \right\rangle$
ein Polynomial Kernel vom Grad 2. Es ist wichtig hervorzuheben, dass bei der Berechnung nie die Funktion $\Theta$ explizit angewendet wurde \cite{handson}
und somit weniger Rechenaufwand ben\"otigt wird. \\































\subsection{K\"unstliche Neuronale Netzwerke}
Wie der Name dieser Modellarchitektur verr\"at, wird bei diesem Machine Learning Ansatz versucht eine Funktion zu bilden, die von der Funktionweise
von biologischen Neuronen in Gehirnen von Menschen und Tieren inspiriert wurde. Die Verbindung von biologischen Neuronen in Form von Synapsen soll durch gewichtete
Verbindungen zwischen den k\"unstlichen Neuronen nachgebildet werden. Die Entstehung von K\"unstlichen Neuronalen Netzwerken 
geht auf McCulloch und Pitts im Jahr 1943 zur\"uck \cite{McCulloch}. \\

K\"unstliche Neuronale Netzwerke stellen eine gro{\ss}e Klasse an Funktionen dar, aus der eine passende Funktion gefunden werden soll, welche 
die wahre Funktion $f$ ausgehend von der Annahme von $Y = f(\mathbf{X}) + \epsilon$ approximieren soll. Die Netzwerke bestehen aus mehreren \textit{Layern} von Neuronen. Der erste Layer hat 
die Dimension des Inputvektors und jedes Neuron des ersten Layers stellt den Wert der jeweiligen Eigenschaft dar. Der letzte Layer in einem Netzwerk hei{\ss}t 
Outputlayer und hat die Dimension der entsprechenden Zielvariable. Die Layer dazwischen werden als \textit{hidden Layer} bezeichnet und sind mit der Anzahl von Neuronen pro 
hidden Layer einer der Hyperparameter die zu jeder Problemstellung angepasst werden m\"ussen. \\

Je nach Anzahl der hidden Layer und der Neuronen pro Layer spricht man von \textit{Deep Neural Networks} oder \textit{Multilayer Perceptrons}. Weiters wird unterschieden, ob
alle Neuronen eines Layers mit allen Neuronen des darauffolgenden Layers verbunden sind oder nicht. Man nennt diese zwei Architekturen \textit{fully-connected} oder \textit{sparse network}. 
Wenn die Gewichte in den Verbindungen zwischen den Layern \"uber mehrere Neuronen geteilt werden, spricht man von \textit{convolutional neural networks} \cite{strang}. \\

Man kann die Vorhersagefunktion $h(\mathbf{x})$ mit ihren Parametern $\mathbf{w}$, in f\"unf Punkte einteilen \cite{strang}. 
Diese sind:
\begin{itemize}
  \item \textbf{Operation}: Verkettung von mehren Funktionen
   $$h(\mathbf{x}, \mathbf{w}) = h_k(h_{k-1}(\dots h_2(h_1(\mathbf{x}, \mathbf{w}_1), \mathbf{w}_2)\dots,\mathbf{w}_{k-1}),\mathbf{w}_k)$$
  \item \textbf{Regel}: Kettenregel um die Ableitungen in den Parametern $\mathbf{w}$ von $F$ zu finden
  \item \textbf{Lernalgorithmus}: \textit{Stochastic Gradient Descent} um die Gewichte anzupassen
  \item \textbf{Vorgehensweise}: \textit{Backpropagation} um die Kettenregel auszuf\"uhren
  \item \textbf{Nichtlinearit\"at}: nichtlineare \textit{Aktivierungsfunktion} $\sigma(\cdot)$
\end{itemize}

Die Funktionen $h_1, h_2, \dots$ stellen jeweils den Output der nacheinandergereihten Layer dar. \\

Der Input in $h_k$ ist der Output von $h_{k-1}$, notiert mit $v_{k-1}$ der L\"ange $n_{k-1}$. Der Output von $F_k$ wird mit $v_k$ notiert und hat L\"ange $n_k$. 
Die Funktion $h_k$ besteht aus einem linearen und einem nichtlinearen Teil:

\begin{itemize}
  \item Der lineare Teil: $A_k v_{k-1} + b_k$ (Hier ist $b_k$ ein bias-Vektor).
  \item Der nichtlineare Teil: Aktivierungsfunktion $\sigma(\cdot)$. Zusammen also:
\end{itemize}
$$ v_k = h_k(v_{k-1}) = \sigma(A_k v_{k-1} + b_k) $$
wobei $\sigma$ komponentenweise angewandt wird.\\

Die Matrix $A_k$ und der bias-Vektor $b_k$ sind die Gewichte an den Verbindungen der einzelnen Neuronen zwischen den Layern $k-1$ und $k$. 
Diese Gewichte werden w\"ahrend des Trainings optimiert. F\"ur die Auswahl von $\sigma$ gibt es mehrere M\"oglichkeiten. Diese Funktion wird an jedem der
$n_k$ Neuronen im Layer $k$ an den Outputs von $A_k v_{k-1} + b_k$ angewandt. \\

Die Aktivierungsfunktion ist auch ein entscheidender Hyperparameter. Die am h\"aufigsten verwendete Aktivierungsfunktion ist die \textit{rectified liner unit function},
auch als \textit{ReLU}-Funktion bekannt \cite{activation}. Sie und ihre erste Ableitung sind folgenderma{\ss}en definiert:
$$ \operatorname{ReLU}(x) = \max(0,x)$$
$$ \frac{d}{dx}\operatorname{ReLU}(x) =
    \left\{
    \begin{array}{lr}
      0,& \text{für }x < 0 \\
      1,& \text{für }x > 0 \\
    \end{array}
    \right. 
$$


In \hyperref[fig:nn]{Abbildung 2.2} ist der Aufbau eines kleinen k\"unstlichen neuronalen Netzwerkes skizziert. Man sieht, dass es einen dreidimensionalen Inputvektor gibt, der
durch das Netzwerk auf einen skalaren Output abgebildet wird. Diese Architektur mit weiteren zwei hidden Layern verdeutlicht, dass es in jedem Neuron zuerst eine Linearkombination der 
Werte der Neuronen des vorherigen Layers gebildet, und anschlie{\ss}en die Aktivierungsfunktion angewendet wird. In diesem Netzwerk sind alle Neuronen eines Layers mit den Neuronen 
des vorherigen und des darauffolgenden Layers verbunden, somit ist es ein fully-connected neuronales Netzwerk. \\

\begin{figure}[ht]
  \label{fig:nn}
  \begin{center}
    \tikzfig{fig2}
  \end{center}
  \caption[K\"unstliches Neuronales Netzwerk]
    {Der Aufbau eines k\"unstlichen neuronalen Netzwerkes mit drei Inputvariablen und einer Outputvariable. Diese Architektur hat zus\"atzlich zu Input- und Outputlayer 
    noch zwei hidden Layer. Diese sind mit $\mathbf{v_k}$ dargestellt. $\mathbf{w_k}$ stellen die zwischenzeitlichen \textit{linearen} Ergebnisse eines jeden Neurons dar. 
    An den blauen Linien werden die Werte der Neuronen mit den Gewichten aus den Matrizen $A_k$ multipliziert. An den schwarzen Linien wird die Aktivierungsfunktion angewandt.}
\end{figure}

Um die Parameter $\mathbf{w}$ des neuronalen Netzwerkes zu trainieren wird versucht, die Summer der Fehler auf den Trainingsdaten zu minimieren. Das bedeutet, dass 
man versucht $\sum_{i=1}^n \ell(y_i, h(\mathbf{x}_i); \mathbf{w})$ zu minimieren. Dies wird mit dem Algorithmus \textit{gradient descent} oder \textit{stochastic gradient descent}
gemacht. Hierf\"ur werden die Gradienten mittels \textit{backpropagation} ermittelt \cite{werbos}. Backpropagation wird auch \textit{reverse-mode automatic differntiation} genannt, und man 
schafft es $n$ partielle Ableitungen mit weniger Rechenaufwand zu berechnen, als mit $n$-mal dem Aufwand einer partiellen Ableitung. Das gelingt, weil man viele 
Zwischenergebnisse immer wieder weiterverwenden kann \cite{strang}.